cmd407: {'sig': ['compact'], 'help': "cause compaction of monitor's leveldb/rocksdb storage", 'module': 'mon', 'perm': 'rw', 'flags': 33}
cmd408: {'sig': ['scrub'], 'help': 'scrub the monitor stores', 'module': 'mon', 'perm': 'rw', 'flags': 2}
cmd409: {'sig': ['fsid'], 'help': 'show cluster FSID/UUID', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd410: {'sig': ['log', {'n': 'N', 'name': 'logtext', 'type': 'CephString'}], 'help': 'log supplied text to the monitor log', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd411: {'sig': ['log', 'last', {'name': 'num', 'range': '1', 'req': 'false', 'type': 'CephInt'}, {'name': 'level', 'req': 'false', 'strings': 'debug|info|sec|warn|error', 'type': 'CephChoices'}, {'name': 'channel', 'req': 'false', 'strings': '*|cluster|audit|cephadm', 'type': 'CephChoices'}], 'help': 'print last few lines of the cluster log', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd412: {'sig': ['status'], 'help': 'show cluster status', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd413: {'sig': ['health', {'name': 'detail', 'req': 'false', 'strings': 'detail', 'type': 'CephChoices'}], 'help': 'show cluster health', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd414: {'sig': ['health', 'mute', {'name': 'code', 'type': 'CephString'}, {'name': 'ttl', 'req': 'false', 'type': 'CephString'}, {'name': 'sticky', 'req': 'false', 'type': 'CephBool'}], 'help': 'mute health alert', 'module': 'mon', 'perm': 'w', 'flags': 0}
cmd415: {'sig': ['health', 'unmute', {'name': 'code', 'req': 'false', 'type': 'CephString'}], 'help': 'unmute existing health alert mute(s)', 'module': 'mon', 'perm': 'w', 'flags': 0}
cmd416: {'sig': ['time-sync-status'], 'help': 'show time sync status', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd417: {'sig': ['df', {'name': 'detail', 'req': 'false', 'strings': 'detail', 'type': 'CephChoices'}], 'help': 'show cluster free space stats', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd418: {'sig': ['report', {'n': 'N', 'name': 'tags', 'req': 'false', 'type': 'CephString'}], 'help': 'report full status of cluster, optional title tag strings', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd419: {'sig': ['features'], 'help': 'report of connected features', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd420: {'sig': ['quorum_status'], 'help': 'report status of monitor quorum', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd424: {'sig': ['tell', {'name': 'target', 'type': 'CephName'}, {'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'send a command to a specific daemon', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd425: {'sig': ['version'], 'help': 'show mon daemon version', 'module': 'mon', 'perm': 'r', 'flags': 33}
cmd426: {'sig': ['node', 'ls', {'name': 'type', 'req': 'false', 'strings': 'all|osd|mon|mds|mgr', 'type': 'CephChoices'}], 'help': 'list all nodes in cluster [type]', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd427: {'sig': ['mon', 'scrub'], 'help': 'scrub the monitor stores', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd428: {'sig': ['mon', 'metadata', {'name': 'id', 'req': 'false', 'type': 'CephString'}], 'help': 'fetch metadata for mon <id>', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd429: {'sig': ['mon', 'count-metadata', {'name': 'property', 'type': 'CephString'}], 'help': 'count mons by metadata field property', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd430: {'sig': ['mon', 'versions'], 'help': 'check running versions of monitors', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd431: {'sig': ['versions'], 'help': 'check running versions of ceph daemons', 'module': 'mon', 'perm': 'r', 'flags': 0}

cmd421: {'sig': ['mon', 'ok-to-stop', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'check whether mon(s) can be safely stopped without reducing immediate availability', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd422: {'sig': ['mon', 'ok-to-add-offline'], 'help': 'check whether adding a mon and not starting it would break quorum', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd423: {'sig': ['mon', 'ok-to-rm', {'name': 'id', 'type': 'CephString'}], 'help': 'check whether removing the specified mon would break quorum', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd472: {'sig': ['mon', 'dump', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'dump formatted monmap (optionally from epoch)', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd473: {'sig': ['mon', 'stat'], 'help': 'summarize monitor status', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd474: {'sig': ['mon', 'getmap', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'get monmap', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd475: {'sig': ['mon', 'add', {'name': 'name', 'type': 'CephString'}, {'name': 'addr', 'type': 'CephIPAddr'}], 'help': 'add new monitor named <name> at <addr>', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd476: {'sig': ['mon', 'rm', {'name': 'name', 'type': 'CephString'}], 'help': 'remove monitor named <name>', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd477: {'sig': ['mon', 'remove', {'name': 'name', 'type': 'CephString'}], 'help': 'remove monitor named <name>', 'module': 'mon', 'perm': 'rw', 'flags': 4}
cmd478: {'sig': ['mon', 'feature', 'ls', {'name': 'with_value', 'req': 'false', 'strings': '--with-value', 'type': 'CephChoices'}], 'help': 'list available mon map features to be set/unset', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd479: {'sig': ['mon', 'feature', 'set', {'name': 'feature_name', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set provided feature on mon map', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd480: {'sig': ['mon', 'set-rank', {'name': 'name', 'type': 'CephString'}, {'name': 'rank', 'type': 'CephInt'}], 'help': 'set the rank for the specified mon', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd481: {'sig': ['mon', 'set-addrs', {'name': 'name', 'type': 'CephString'}, {'name': 'addrs', 'type': 'CephString'}], 'help': 'set the addrs (IPs and ports) a specific monitor binds to', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd482: {'sig': ['mon', 'set-weight', {'name': 'name', 'type': 'CephString'}, {'name': 'weight', 'range': '0|65535', 'type': 'CephInt'}], 'help': 'set the weight for the specified mon', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd483: {'sig': ['mon', 'enable-msgr2'], 'help': 'enable the msgr2 protocol on port 3300', 'module': 'mon', 'perm': 'rw', 'flags': 0}
cmd484: {'sig': ['osd', 'stat'], 'help': 'print summary of OSD map', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd485: {'sig': ['osd', 'dump', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'print summary of OSD map', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd486: {'sig': ['osd', 'info', {'name': 'id', 'req': 'false', 'type': 'CephOsdName'}], 'help': "print osd's {id} information (instead of all osds from map)", 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd487: {'sig': ['osd', 'tree', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'n': 'N', 'name': 'states', 'req': 'false', 'strings': 'up|down|in|out|destroyed', 'type': 'CephChoices'}], 'help': 'print OSD tree', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd488: {'sig': ['osd', 'tree-from', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'bucket', 'type': 'CephString'}, {'n': 'N', 'name': 'states', 'req': 'false', 'strings': 'up|down|in|out|destroyed', 'type': 'CephChoices'}], 'help': 'print OSD tree in bucket', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd489: {'sig': ['osd', 'ls', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'show all OSD ids', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd490: {'sig': ['osd', 'getmap', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'get OSD map', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd491: {'sig': ['osd', 'getcrushmap', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'get CRUSH map', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd492: {'sig': ['osd', 'getmaxosd'], 'help': 'show largest OSD id', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd493: {'sig': ['osd', 'ls-tree', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'name', 'req': 'true', 'type': 'CephString'}], 'help': 'show OSD ids under bucket <name> in the CRUSH map', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd494: {'sig': ['osd', 'find', {'name': 'id', 'type': 'CephOsdName'}], 'help': 'find osd <id> in the CRUSH map and show its location', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd495: {'sig': ['osd', 'metadata', {'name': 'id', 'req': 'false', 'type': 'CephOsdName'}], 'help': 'fetch metadata for osd {id} (default all)', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd496: {'sig': ['osd', 'count-metadata', {'name': 'property', 'type': 'CephString'}], 'help': 'count OSDs by metadata field property', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd497: {'sig': ['osd', 'versions'], 'help': 'check running versions of OSDs', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd498: {'sig': ['osd', 'numa-status'], 'help': 'show NUMA status of OSDs', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd499: {'sig': ['osd', 'map', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'object', 'type': 'CephObjectname'}, {'name': 'nspace', 'req': 'false', 'type': 'CephString'}], 'help': 'find pg for <object> in <pool> with [namespace]', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd500: {'sig': ['osd', 'lspools'], 'help': 'list pools', 'module': 'osd', 'perm': 'r', 'flags': 4}

cmd501: {'sig': ['osd', 'crush', 'rule', 'list'], 'help': 'list crush rules', 'module': 'osd', 'perm': 'r', 'flags': 4}
cmd502: {'sig': ['osd', 'crush', 'rule', 'ls'], 'help': 'list crush rules', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd503: {'sig': ['osd', 'crush', 'rule', 'ls-by-class', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'class', 'type': 'CephString'}], 'help': 'list all crush rules that reference the same <class>', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd504: {'sig': ['osd', 'crush', 'rule', 'dump', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'req': 'false', 'type': 'CephString'}], 'help': 'dump crush rule <name> (default all)', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd505: {'sig': ['osd', 'crush', 'dump'], 'help': 'dump crush map', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd506: {'sig': ['osd', 'setcrushmap', {'name': 'prior_version', 'req': 'false', 'type': 'CephInt'}], 'help': 'set crush map from input file', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd507: {'sig': ['osd', 'crush', 'set', {'name': 'prior_version', 'req': 'false', 'type': 'CephInt'}], 'help': 'set crush map from input file', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd508: {'sig': ['osd', 'crush', 'add-bucket', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'name': 'type', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.=]', 'n': 'N', 'name': 'args', 'req': 'false', 'type': 'CephString'}], 'help': 'add no-parent (probably root) crush bucket <name> of type <type> to location <args>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd509: {'sig': ['osd', 'crush', 'rename-bucket', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'srcname', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'dstname', 'type': 'CephString'}], 'help': 'rename bucket <srcname> to <dstname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd510: {'sig': ['osd', 'crush', 'set', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}, {'goodchars': '[A-Za-z0-9-_.=]', 'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'update crushmap position and weight for <name> to <weight> with location <args>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd511: {'sig': ['osd', 'crush', 'add', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}, {'goodchars': '[A-Za-z0-9-_.=]', 'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'add or update crushmap position and weight for <name> with <weight> and location <args>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd512: {'sig': ['osd', 'crush', 'set-all-straw-buckets-to-straw2'], 'help': 'convert all CRUSH current straw buckets to use the straw2 algorithm', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd513: {'sig': ['osd', 'crush', 'class', 'create', {'goodchars': '[A-Za-z0-9-_]', 'name': 'class', 'type': 'CephString'}], 'help': 'create crush device class <class>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd514: {'sig': ['osd', 'crush', 'class', 'rm', {'goodchars': '[A-Za-z0-9-_]', 'name': 'class', 'type': 'CephString'}], 'help': 'remove crush device class <class>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd515: {'sig': ['osd', 'crush', 'set-device-class', {'name': 'class', 'type': 'CephString'}, {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'set the <class> of the osd(s) <id> [<id>...],or use <all|any> to set all.', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd516: {'sig': ['osd', 'crush', 'rm-device-class', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'remove class of the osd(s) <id> [<id>...],or use <all|any> to remove all.', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd517: {'sig': ['osd', 'crush', 'class', 'rename', {'goodchars': '[A-Za-z0-9-_]', 'name': 'srcname', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_]', 'name': 'dstname', 'type': 'CephString'}], 'help': 'rename crush device class <srcname> to <dstname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd518: {'sig': ['osd', 'crush', 'create-or-move', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}, {'goodchars': '[A-Za-z0-9-_.=]', 'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'create entry or move existing entry for <name> <weight> at/to location <args>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd519: {'sig': ['osd', 'crush', 'move', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.=]', 'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'move existing entry for <name> to location <args>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd520: {'sig': ['osd', 'crush', 'swap-bucket', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'source', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'dest', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'swap existing bucket contents from (orphan) bucket <source> and <target>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd521: {'sig': ['osd', 'crush', 'link', {'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.=]', 'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'link existing entry for <name> under location <args>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd522: {'sig': ['osd', 'crush', 'rm', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'ancestor', 'req': 'false', 'type': 'CephString'}], 'help': 'remove <name> from crush map (everywhere, or just at <ancestor>)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd523: {'sig': ['osd', 'crush', 'remove', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'ancestor', 'req': 'false', 'type': 'CephString'}], 'help': 'remove <name> from crush map (everywhere, or just at <ancestor>)', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd524: {'sig': ['osd', 'crush', 'unlink', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'ancestor', 'req': 'false', 'type': 'CephString'}], 'help': 'unlink <name> from crush map (everywhere, or just at <ancestor>)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd525: {'sig': ['osd', 'crush', 'reweight-all'], 'help': 'recalculate the weights for the tree to ensure they sum correctly', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd526: {'sig': ['osd', 'crush', 'reweight', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}], 'help': "change <name>'s weight to <weight> in crush map", 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd527: {'sig': ['osd', 'crush', 'reweight-subtree', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}], 'help': 'change all leaf items beneath <name> to <weight> in crush map', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd528: {'sig': ['osd', 'crush', 'tunables', {'name': 'profile', 'strings': 'legacy|argonaut|bobtail|firefly|hammer|jewel|optimal|default', 'type': 'CephChoices'}], 'help': 'set crush tunables values to <profile>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd529: {'sig': ['osd', 'crush', 'set-tunable', {'name': 'tunable', 'strings': 'straw_calc_version', 'type': 'CephChoices'}, {'name': 'value', 'type': 'CephInt'}], 'help': 'set crush tunable <tunable> to <value>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd530: {'sig': ['osd', 'crush', 'get-tunable', {'name': 'tunable', 'strings': 'straw_calc_version', 'type': 'CephChoices'}], 'help': 'get crush tunable <tunable>', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd531: {'sig': ['osd', 'crush', 'show-tunables'], 'help': 'show current crush tunables', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd532: {'sig': ['osd', 'crush', 'rule', 'create-simple', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'root', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'type', 'type': 'CephString'}, {'name': 'mode', 'req': 'false', 'strings': 'firstn|indep', 'type': 'CephChoices'}], 'help': 'create crush rule <name> to start from <root>, replicate across buckets of type <type>, using a choose mode of <firstn|indep> (default firstn; indep best for erasure pools)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd533: {'sig': ['osd', 'crush', 'rule', 'create-replicated', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'root', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'type', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'class', 'req': 'false', 'type': 'CephString'}], 'help': 'create crush rule <name> for replicated pool to start from <root>, replicate across buckets of type <type>, use devices of type <class> (ssd or hdd)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd534: {'sig': ['osd', 'crush', 'rule', 'create-erasure', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.=]', 'name': 'profile', 'req': 'false', 'type': 'CephString'}], 'help': 'create crush rule <name> for erasure coded pool created with <profile> (default default)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd535: {'sig': ['osd', 'crush', 'rule', 'rm', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}], 'help': 'remove crush rule <name>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd536: {'sig': ['osd', 'crush', 'rule', 'rename', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'srcname', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'dstname', 'type': 'CephString'}], 'help': 'rename crush rule <srcname> to <dstname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd537: {'sig': ['osd', 'crush', 'tree', {'name': 'shadow', 'req': 'false', 'strings': '--show-shadow', 'type': 'CephChoices'}], 'help': 'dump crush buckets and items in a tree view', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd538: {'sig': ['osd', 'crush', 'ls', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'node', 'type': 'CephString'}], 'help': 'list items beneath a node in the CRUSH tree', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd539: {'sig': ['osd', 'crush', 'class', 'ls'], 'help': 'list all crush device classes', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd540: {'sig': ['osd', 'crush', 'class', 'ls-osd', {'goodchars': '[A-Za-z0-9-_]', 'name': 'class', 'type': 'CephString'}], 'help': 'list all osds belonging to the specific <class>', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd541: {'sig': ['osd', 'crush', 'get-device-class', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'get classes of specified osd(s) <id> [<id>...]', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd542: {'sig': ['osd', 'crush', 'weight-set', 'ls'], 'help': 'list crush weight sets', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd543: {'sig': ['osd', 'crush', 'weight-set', 'dump'], 'help': 'dump crush weight sets', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd544: {'sig': ['osd', 'crush', 'weight-set', 'create-compat'], 'help': 'create a default backward-compatible weight-set', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd545: {'sig': ['osd', 'crush', 'weight-set', 'create', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'mode', 'strings': 'flat|positional', 'type': 'CephChoices'}], 'help': 'create a weight-set for a given pool', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd546: {'sig': ['osd', 'crush', 'weight-set', 'rm', {'name': 'pool', 'type': 'CephPoolname'}], 'help': 'remove the weight-set for a given pool', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd547: {'sig': ['osd', 'crush', 'weight-set', 'rm-compat'], 'help': 'remove the backward-compatible weight-set', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd548: {'sig': ['osd', 'crush', 'weight-set', 'reweight', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'item', 'type': 'CephString'}, {'n': 'N', 'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}], 'help': "set weight for an item (bucket or osd) in a pool's weight-set", 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd549: {'sig': ['osd', 'crush', 'weight-set', 'reweight-compat', {'name': 'item', 'type': 'CephString'}, {'n': 'N', 'name': 'weight', 'range': '0.0', 'type': 'CephFloat'}], 'help': 'set weight for an item (bucket or osd) in the backward-compatible weight-set', 'module': 'osd', 'perm': 'rw', 'flags': 0}

cmd550: {'sig': ['osd', 'setmaxosd', {'name': 'newmax', 'range': '0', 'type': 'CephInt'}], 'help': 'set new maximum osd value', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd551: {'sig': ['osd', 'set-full-ratio', {'name': 'ratio', 'range': '0.0|1.0', 'type': 'CephFloat'}], 'help': 'set usage ratio at which OSDs are marked full', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd552: {'sig': ['osd', 'set-backfillfull-ratio', {'name': 'ratio', 'range': '0.0|1.0', 'type': 'CephFloat'}], 'help': 'set usage ratio at which OSDs are marked too full to backfill', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd553: {'sig': ['osd', 'set-nearfull-ratio', {'name': 'ratio', 'range': '0.0|1.0', 'type': 'CephFloat'}], 'help': 'set usage ratio at which OSDs are marked near-full', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd554: {'sig': ['osd', 'get-require-min-compat-client'], 'help': 'get the minimum client version we will maintain compatibility with', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd555: {'sig': ['osd', 'set-require-min-compat-client', {'name': 'version', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set the minimum client version we will maintain compatibility with', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd556: {'sig': ['osd', 'pause'], 'help': 'pause osd', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd557: {'sig': ['osd', 'unpause'], 'help': 'unpause osd', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd558: {'sig': ['osd', 'erasure-code-profile', 'set', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}, {'n': 'N', 'name': 'profile', 'req': 'false', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}], 'help': 'create erasure code profile <name> with [<key[=value]> ...] pairs. Add a --force at the end to override an existing profile (VERY DANGEROUS)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd559: {'sig': ['osd', 'erasure-code-profile', 'get', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}], 'help': 'get erasure code profile <name>', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd560: {'sig': ['osd', 'erasure-code-profile', 'rm', {'goodchars': '[A-Za-z0-9-_.]', 'name': 'name', 'type': 'CephString'}], 'help': 'remove erasure code profile <name>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd561: {'sig': ['osd', 'erasure-code-profile', 'ls'], 'help': 'list all erasure code profiles', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd562: {'sig': ['osd', 'set', {'name': 'key', 'strings': 'full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|notieragent|nosnaptrim|pglog_hardlimit', 'type': 'CephChoices'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set <key>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd563: {'sig': ['osd', 'unset', {'name': 'key', 'strings': 'full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|notieragent|nosnaptrim', 'type': 'CephChoices'}], 'help': 'unset <key>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd564: {'sig': ['osd', 'require-osd-release', {'name': 'release', 'strings': 'luminous|mimic|nautilus|octopus', 'type': 'CephChoices'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set the minimum allowed OSD release to participate in the cluster', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd565: {'sig': ['osd', 'down', {'n': 'N', 'name': 'ids', 'type': 'CephString'}, {'name': 'definitely_dead', 'req': 'false', 'type': 'CephBool'}], 'help': 'set osd(s) <id> [<id>...] down, or use <any|all> to set all osds down', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd566: {'sig': ['osd', 'stop', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'stop the corresponding osd daemons and mark them as down', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd567: {'sig': ['osd', 'out', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'set osd(s) <id> [<id>...] out, or use <any|all> to set all osds out', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd568: {'sig': ['osd', 'in', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'set osd(s) <id> [<id>...] in, can use <any|all> to automatically set all previously out osds in', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd569: {'sig': ['osd', 'rm', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'remove osd(s) <id> [<id>...], or use <any|all> to remove all osds', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd570: {'sig': ['osd', 'add-noup', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'mark osd(s) <id> [<id>...] as noup, or use <all|any> to mark all osds as noup', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd571: {'sig': ['osd', 'add-nodown', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'mark osd(s) <id> [<id>...] as nodown, or use <all|any> to mark all osds as nodown', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd572: {'sig': ['osd', 'add-noin', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'mark osd(s) <id> [<id>...] as noin, or use <all|any> to mark all osds as noin', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd573: {'sig': ['osd', 'add-noout', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'mark osd(s) <id> [<id>...] as noout, or use <all|any> to mark all osds as noout', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd574: {'sig': ['osd', 'rm-noup', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'allow osd(s) <id> [<id>...] to be marked up (if they are currently marked as noup), can use <all|any> to automatically filter out all noup osds', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd575: {'sig': ['osd', 'rm-nodown', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'allow osd(s) <id> [<id>...] to be marked down (if they are currently marked as nodown), can use <all|any> to automatically filter out all nodown osds', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd576: {'sig': ['osd', 'rm-noin', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'allow osd(s) <id> [<id>...] to be marked in (if they are currently marked as noin), can use <all|any> to automatically filter out all noin osds', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd577: {'sig': ['osd', 'rm-noout', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'allow osd(s) <id> [<id>...] to be marked out (if they are currently marked as noout), can use <all|any> to automatically filter out all noout osds', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd578: {'sig': ['osd', 'set-group', {'name': 'flags', 'type': 'CephString'}, {'n': 'N', 'name': 'who', 'type': 'CephString'}], 'help': 'set <flags> for batch osds or crush nodes, <flags> must be a comma-separated subset of {noup,nodown,noin,noout}', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd579: {'sig': ['osd', 'unset-group', {'name': 'flags', 'type': 'CephString'}, {'n': 'N', 'name': 'who', 'type': 'CephString'}], 'help': 'unset <flags> for batch osds or crush nodes, <flags> must be a comma-separated subset of {noup,nodown,noin,noout}', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd580: {'sig': ['osd', 'reweight', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'weight', 'range': '0.0|1.0', 'type': 'CephFloat'}], 'help': 'reweight osd to 0.0 < <weight> < 1.0', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd581: {'sig': ['osd', 'reweightn', {'name': 'weights', 'type': 'CephString'}], 'help': 'reweight osds with {<id>: <weight>,...})', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd582: {'sig': ['osd', 'force-create-pg', {'name': 'pgid', 'type': 'CephPgid'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'force creation of pg <pgid>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd583: {'sig': ['osd', 'pg-temp', {'name': 'pgid', 'type': 'CephPgid'}, {'n': 'N', 'name': 'id', 'req': 'false', 'type': 'CephOsdName'}], 'help': 'set pg_temp mapping pgid:[<id> [<id>...]] (developers only)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd584: {'sig': ['osd', 'pg-upmap', {'name': 'pgid', 'type': 'CephPgid'}, {'n': 'N', 'name': 'id', 'type': 'CephOsdName'}], 'help': 'set pg_upmap mapping <pgid>:[<id> [<id>...]] (developers only)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd585: {'sig': ['osd', 'rm-pg-upmap', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'clear pg_upmap mapping for <pgid> (developers only)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd586: {'sig': ['osd', 'pg-upmap-items', {'name': 'pgid', 'type': 'CephPgid'}, {'n': 'N', 'name': 'id', 'type': 'CephOsdName'}], 'help': 'set pg_upmap_items mapping <pgid>:{<id> to <id>, [...]} (developers only)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd587: {'sig': ['osd', 'rm-pg-upmap-items', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'clear pg_upmap_items mapping for <pgid> (developers only)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd588: {'sig': ['osd', 'primary-temp', {'name': 'pgid', 'type': 'CephPgid'}, {'name': 'id', 'type': 'CephOsdName'}], 'help': 'set primary_temp mapping pgid:<id>|-1 (developers only)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd589: {'sig': ['osd', 'primary-affinity', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'weight', 'range': '0.0|1.0', 'type': 'CephFloat'}], 'help': 'adjust osd primary-affinity from 0.0 <= <weight> <= 1.0', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd590: {'sig': ['osd', 'destroy-actual', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'mark osd as being destroyed. Keeps the ID intact (allowing reuse), but removes cephx keys, config-key data and lockbox keys, rendering data permanently unreadable.', 'module': 'osd', 'perm': 'rw', 'flags': 32}
cmd591: {'sig': ['osd', 'purge-new', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'purge all traces of an OSD that was partially created but never started', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd592: {'sig': ['osd', 'purge-actual', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'purge all osd data from the monitors. Combines `osd destroy`, `osd rm`, and `osd crush rm`.', 'module': 'osd', 'perm': 'rw', 'flags': 32}
cmd593: {'sig': ['osd', 'lost', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'mark osd as permanently lost. THIS DESTROYS DATA IF NO MORE REPLICAS EXIST, BE CAREFUL', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd594: {'sig': ['osd', 'create', {'name': 'uuid', 'req': 'false', 'type': 'CephUUID'}, {'name': 'id', 'req': 'false', 'type': 'CephOsdName'}], 'help': 'create new osd (with optional UUID and ID)', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd595: {'sig': ['osd', 'new', {'name': 'uuid', 'req': 'true', 'type': 'CephUUID'}, {'name': 'id', 'req': 'false', 'type': 'CephOsdName'}], 'help': 'Create a new OSD. If supplied, the `id` to be replaced needs to exist and have been previously destroyed. Reads secrets from JSON file via `-i <file>` (see man page).', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd596: {'sig': ['osd', 'blacklist', {'name': 'blacklistop', 'strings': 'add|rm', 'type': 'CephChoices'}, {'name': 'addr', 'type': 'CephEntityAddr'}, {'name': 'expire', 'range': '0.0', 'req': 'false', 'type': 'CephFloat'}], 'help': 'add (optionally until <expire> seconds from now) or remove <addr> from blacklist', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd597: {'sig': ['osd', 'blacklist', 'ls'], 'help': 'show blacklisted clients', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd598: {'sig': ['osd', 'blacklist', 'clear'], 'help': 'clear all blacklisted clients', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd599: {'sig': ['osd', 'pool', 'mksnap', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'snap', 'type': 'CephString'}], 'help': 'make snapshot <snap> in <pool>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd600: {'sig': ['osd', 'pool', 'rmsnap', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'snap', 'type': 'CephString'}], 'help': 'remove snapshot <snap> from <pool>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd601: {'sig': ['osd', 'pool', 'ls', {'name': 'detail', 'req': 'false', 'strings': 'detail', 'type': 'CephChoices'}], 'help': 'list pools', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd602: {'sig': ['osd', 'pool', 'create', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'pg_num', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'pgp_num', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'pool_type', 'req': 'false', 'strings': 'replicated|erasure', 'type': 'CephChoices'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'erasure_code_profile', 'req': 'false', 'type': 'CephString'}, {'name': 'rule', 'req': 'false', 'type': 'CephString'}, {'name': 'expected_num_objects', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'size', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'pg_num_min', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'autoscale_mode', 'req': 'false', 'strings': 'on|off|warn', 'type': 'CephChoices'}, {'name': 'target_size_bytes', 'range': '0', 'req': 'false', 'type': 'CephInt'}, {'name': 'target_size_ratio', 'range': '0|1', 'req': 'false', 'type': 'CephFloat'}], 'help': 'create pool', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd603: {'sig': ['osd', 'pool', 'delete', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'pool2', 'req': 'false', 'type': 'CephPoolname'}, {'name': 'yes_i_really_really_mean_it', 'req': 'false', 'type': 'CephBool'}, {'name': 'yes_i_really_really_mean_it_not_faking', 'req': 'false', 'type': 'CephBool'}], 'help': 'delete pool', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd604: {'sig': ['osd', 'pool', 'rm', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'pool2', 'req': 'false', 'type': 'CephPoolname'}, {'name': 'yes_i_really_really_mean_it', 'req': 'false', 'type': 'CephBool'}, {'name': 'yes_i_really_really_mean_it_not_faking', 'req': 'false', 'type': 'CephBool'}], 'help': 'remove pool', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd605: {'sig': ['osd', 'pool', 'rename', {'name': 'srcpool', 'type': 'CephPoolname'}, {'name': 'destpool', 'type': 'CephPoolname'}], 'help': 'rename <srcpool> to <destpool>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd606: {'sig': ['osd', 'pool', 'get', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'var', 'strings': 'size|min_size|pg_num|pgp_num|crush_rule|hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|hit_set_fpp|use_gmt_hitset|target_max_objects|target_max_bytes|cache_target_dirty_ratio|cache_target_dirty_high_ratio|cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age|erasure_code_profile|min_read_recency_for_promote|all|min_write_recency_for_promote|fast_read|hit_set_grade_decay_rate|hit_set_search_last_n|scrub_min_interval|scrub_max_interval|deep_scrub_interval|recovery_priority|recovery_op_priority|scrub_priority|compression_mode|compression_algorithm|compression_required_ratio|compression_max_blob_size|compression_min_blob_size|csum_type|csum_min_block|csum_max_block|allow_ec_overwrites|fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min|target_size_bytes|target_size_ratio', 'type': 'CephChoices'}], 'help': 'get pool parameter <var>', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd607: {'sig': ['osd', 'pool', 'set', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'var', 'strings': 'size|min_size|pg_num|pgp_num|pgp_num_actual|crush_rule|hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|hit_set_fpp|use_gmt_hitset|target_max_bytes|target_max_objects|cache_target_dirty_ratio|cache_target_dirty_high_ratio|cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age|min_read_recency_for_promote|min_write_recency_for_promote|fast_read|hit_set_grade_decay_rate|hit_set_search_last_n|scrub_min_interval|scrub_max_interval|deep_scrub_interval|recovery_priority|recovery_op_priority|scrub_priority|compression_mode|compression_algorithm|compression_required_ratio|compression_max_blob_size|compression_min_blob_size|csum_type|csum_min_block|csum_max_block|allow_ec_overwrites|fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min|target_size_bytes|target_size_ratio', 'type': 'CephChoices'}, {'name': 'val', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set pool parameter <var> to <val>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd608: {'sig': ['osd', 'pool', 'set-quota', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'field', 'strings': 'max_objects|max_bytes', 'type': 'CephChoices'}, {'name': 'val', 'type': 'CephString'}], 'help': 'set object or byte limit on pool', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd609: {'sig': ['osd', 'pool', 'get-quota', {'name': 'pool', 'type': 'CephPoolname'}], 'help': 'obtain object or byte limits for pool', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd610: {'sig': ['osd', 'pool', 'application', 'enable', {'name': 'pool', 'type': 'CephPoolname'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'app', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'enable use of an application <app> [cephfs,rbd,rgw] on pool <poolname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd611: {'sig': ['osd', 'pool', 'application', 'disable', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'app', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'disables use of an application <app> on pool <poolname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd612: {'sig': ['osd', 'pool', 'application', 'set', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'app', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'key', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.=]', 'name': 'value', 'type': 'CephString'}], 'help': 'sets application <app> metadata key <key> to <value> on pool <poolname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd613: {'sig': ['osd', 'pool', 'application', 'rm', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'app', 'type': 'CephString'}, {'name': 'key', 'type': 'CephString'}], 'help': 'removes application <app> metadata key <key> on pool <poolname>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd614: {'sig': ['osd', 'pool', 'application', 'get', {'name': 'pool', 'req': 'fasle', 'type': 'CephPoolname'}, {'name': 'app', 'req': 'false', 'type': 'CephString'}, {'name': 'key', 'req': 'false', 'type': 'CephString'}], 'help': 'get value of key <key> of application <app> on pool <poolname>', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd021: {'sig': ['osd', 'pool', 'stats', {'name': 'pool_name', 'req': 'false', 'type': 'CephPoolname'}], 'help': 'obtain stats from all pools, or from specified pool', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd022: {'sig': ['osd', 'pool', 'scrub', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'initiate scrub on pool <who>', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd023: {'sig': ['osd', 'pool', 'deep-scrub', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'initiate deep-scrub on pool <who>', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd024: {'sig': ['osd', 'pool', 'repair', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'initiate repair on pool <who>', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd025: {'sig': ['osd', 'pool', 'force-recovery', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'force recovery of specified pool <who> first', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd026: {'sig': ['osd', 'pool', 'force-backfill', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'force backfill of specified pool <who> first', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd027: {'sig': ['osd', 'pool', 'cancel-force-recovery', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'restore normal recovery priority of specified pool <who>', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd028: {'sig': ['osd', 'pool', 'cancel-force-backfill', {'n': 'N', 'name': 'who', 'type': 'CephPoolname'}], 'help': 'restore normal recovery priority of specified pool <who>', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd029: {'sig': ['osd', 'reweight-by-utilization', {'name': 'oload', 'req': 'false', 'type': 'CephInt'}, {'name': 'max_change', 'req': 'false', 'type': 'CephFloat'}, {'name': 'max_osds', 'req': 'false', 'type': 'CephInt'}, {'name': 'no_increasing', 'req': 'false', 'strings': '--no-increasing', 'type': 'CephChoices'}], 'help': 'reweight OSDs by utilization [overload-percentage-for-consideration, default 120]', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd030: {'sig': ['osd', 'test-reweight-by-utilization', {'name': 'oload', 'req': 'false', 'type': 'CephInt'}, {'name': 'max_change', 'req': 'false', 'type': 'CephFloat'}, {'name': 'max_osds', 'req': 'false', 'type': 'CephInt'}, {'name': 'no_increasing', 'req': 'false', 'type': 'CephBool'}], 'help': 'dry run of reweight OSDs by utilization [overload-percentage-for-consideration, default 120]', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd031: {'sig': ['osd', 'reweight-by-pg', {'name': 'oload', 'req': 'false', 'type': 'CephInt'}, {'name': 'max_change', 'req': 'false', 'type': 'CephFloat'}, {'name': 'max_osds', 'req': 'false', 'type': 'CephInt'}, {'n': 'N', 'name': 'pools', 'req': 'false', 'type': 'CephPoolname'}], 'help': 'reweight OSDs by PG distribution [overload-percentage-for-consideration, default 120]', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd032: {'sig': ['osd', 'test-reweight-by-pg', {'name': 'oload', 'req': 'false', 'type': 'CephInt'}, {'name': 'max_change', 'req': 'false', 'type': 'CephFloat'}, {'name': 'max_osds', 'req': 'false', 'type': 'CephInt'}, {'n': 'N', 'name': 'pools', 'req': 'false', 'type': 'CephPoolname'}], 'help': 'dry run of reweight OSDs by PG distribution [overload-percentage-for-consideration, default 120]', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd033: {'sig': ['osd', 'destroy', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'mark osd as being destroyed. Keeps the ID intact (allowing reuse), but removes cephx keys, config-key data and lockbox keys, rendering data permanently unreadable.', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd034: {'sig': ['osd', 'purge', {'name': 'id', 'type': 'CephOsdName'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'purge all osd data from the monitors including the OSD id and CRUSH position', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd035: {'sig': ['osd', 'safe-to-destroy', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'check whether osd(s) can be safely destroyed without reducing data durability', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd036: {'sig': ['osd', 'ok-to-stop', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'check whether osd(s) can be safely stopped without reducing immediate data availability', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd037: {'sig': ['osd', 'scrub', {'name': 'who', 'type': 'CephString'}], 'help': 'initiate scrub on osd <who>, or use <all|any> to scrub all', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd038: {'sig': ['osd', 'deep-scrub', {'name': 'who', 'type': 'CephString'}], 'help': 'initiate deep scrub on osd <who>, or use <all|any> to deep scrub all', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd039: {'sig': ['osd', 'repair', {'name': 'who', 'type': 'CephString'}], 'help': 'initiate repair on osd <who>, or use <all|any> to repair all', 'module': 'osd', 'perm': 'rw', 'flags': 8}
cmd615: {'sig': ['osd', 'utilization'], 'help': 'get basic pg distribution stats', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd616: {'sig': ['osd', 'tier', 'add', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'tierpool', 'type': 'CephPoolname'}, {'name': 'force_nonempty', 'req': 'false', 'strings': '--force-nonempty', 'type': 'CephChoices'}], 'help': 'add the tier <tierpool> (the second one) to base pool <pool> (the first one)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd617: {'sig': ['osd', 'tier', 'rm', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'tierpool', 'type': 'CephPoolname'}], 'help': 'remove the tier <tierpool> (the second one) from base pool <pool> (the first one)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd618: {'sig': ['osd', 'tier', 'remove', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'tierpool', 'type': 'CephPoolname'}], 'help': 'remove the tier <tierpool> (the second one) from base pool <pool> (the first one)', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd619: {'sig': ['osd', 'tier', 'cache-mode', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'mode', 'strings': 'writeback|readproxy|readonly|none', 'type': 'CephChoices'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'specify the caching mode for cache tier <pool>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd620: {'sig': ['osd', 'tier', 'set-overlay', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'overlaypool', 'type': 'CephPoolname'}], 'help': 'set the overlay pool for base pool <pool> to be <overlaypool>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd621: {'sig': ['osd', 'tier', 'rm-overlay', {'name': 'pool', 'type': 'CephPoolname'}], 'help': 'remove the overlay pool for base pool <pool>', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd622: {'sig': ['osd', 'tier', 'remove-overlay', {'name': 'pool', 'type': 'CephPoolname'}], 'help': 'remove the overlay pool for base pool <pool>', 'module': 'osd', 'perm': 'rw', 'flags': 4}
cmd623: {'sig': ['osd', 'tier', 'add-cache', {'name': 'pool', 'type': 'CephPoolname'}, {'name': 'tierpool', 'type': 'CephPoolname'}, {'name': 'size', 'range': '0', 'type': 'CephInt'}], 'help': 'add a cache <tierpool> (the second one) of size <size> to existing pool <pool> (the first one)', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd018: {'sig': ['osd', 'perf'], 'help': 'print dump of OSD perf summary stats', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd019: {'sig': ['osd', 'df', {'name': 'output_method', 'req': 'false', 'strings': 'plain|tree', 'type': 'CephChoices'}, {'name': 'filter_by', 'req': 'false', 'strings': 'class|name', 'type': 'CephChoices'}, {'name': 'filter', 'req': 'false', 'type': 'CephString'}], 'help': 'show OSD utilization', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd020: {'sig': ['osd', 'blocked-by'], 'help': 'print histogram of which OSDs are blocking their peers', 'module': 'osd', 'perm': 'r', 'flags': 8}
cmd391: {'sig': ['osd', 'last-stat-seq', {'name': 'id', 'type': 'CephOsdName'}], 'help': 'get the last pg stats sequence number reported for this osd', 'module': 'osd', 'perm': 'r', 'flags': 0}
cmd331: {'sig': ['osd', 'status', {'name': 'bucket', 'req': 'false', 'type': 'CephString'}], 'help': 'Show the status of OSDs within a bucket, or all', 'module': 'mgr', 'perm': 'r', 'flags': 8}


cmd389: {'sig': ['pg', 'map', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'show mapping of pg to osds', 'module': 'pg', 'perm': 'r', 'flags': 0}
cmd390: {'sig': ['pg', 'repeer', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'force a PG to repeer', 'module': 'osd', 'perm': 'rw', 'flags': 0}
cmd000: {'sig': ['pg', 'stat'], 'help': 'show placement group status.', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd001: {'sig': ['pg', 'getmap'], 'help': 'get binary pg map to -o/stdout', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd002: {'sig': ['pg', 'dump', {'n': 'N', 'name': 'dumpcontents', 'req': 'false', 'strings': 'all|summary|sum|delta|pools|osds|pgs|pgs_brief', 'type': 'CephChoices'}], 'help': "show human-readable versions of pg map (only 'all' valid with plain)", 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd003: {'sig': ['pg', 'dump_json', {'n': 'N', 'name': 'dumpcontents', 'req': 'false', 'strings': 'all|summary|sum|pools|osds|pgs', 'type': 'CephChoices'}], 'help': 'show human-readable version of pg map in json only', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd004: {'sig': ['pg', 'dump_pools_json'], 'help': 'show pg pools info in json only', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd005: {'sig': ['pg', 'ls-by-pool', {'name': 'poolstr', 'type': 'CephString'}, {'n': 'N', 'name': 'states', 'req': 'false', 'type': 'CephString'}], 'help': 'list pg with pool = [poolname]', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd006: {'sig': ['pg', 'ls-by-primary', {'name': 'osd', 'type': 'CephOsdName'}, {'name': 'pool', 'req': 'false', 'type': 'CephInt'}, {'n': 'N', 'name': 'states', 'req': 'false', 'type': 'CephString'}], 'help': 'list pg with primary = [osd]', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd007: {'sig': ['pg', 'ls-by-osd', {'name': 'osd', 'type': 'CephOsdName'}, {'name': 'pool', 'req': 'false', 'type': 'CephInt'}, {'n': 'N', 'name': 'states', 'req': 'false', 'type': 'CephString'}], 'help': 'list pg on osd [osd]', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd008: {'sig': ['pg', 'ls', {'name': 'pool', 'req': 'false', 'type': 'CephInt'}, {'n': 'N', 'name': 'states', 'req': 'false', 'type': 'CephString'}], 'help': 'list pg with specific pool, osd, state', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd009: {'sig': ['pg', 'dump_stuck', {'n': 'N', 'name': 'stuckops', 'req': 'false', 'strings': 'inactive|unclean|stale|undersized|degraded', 'type': 'CephChoices'}, {'name': 'threshold', 'req': 'false', 'type': 'CephInt'}], 'help': 'show information about stuck pgs', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd010: {'sig': ['pg', 'debug', {'name': 'debugop', 'strings': 'unfound_objects_exist|degraded_pgs_exist', 'type': 'CephChoices'}], 'help': 'show debug info about pgs', 'module': 'pg', 'perm': 'r', 'flags': 8}
cmd011: {'sig': ['pg', 'scrub', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'start scrub on <pgid>', 'module': 'pg', 'perm': 'rw', 'flags': 8}
cmd012: {'sig': ['pg', 'deep-scrub', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'start deep-scrub on <pgid>', 'module': 'pg', 'perm': 'rw', 'flags': 8}
cmd013: {'sig': ['pg', 'repair', {'name': 'pgid', 'type': 'CephPgid'}], 'help': 'start repair on <pgid>', 'module': 'pg', 'perm': 'rw', 'flags': 8}
cmd014: {'sig': ['pg', 'force-recovery', {'n': 'N', 'name': 'pgid', 'type': 'CephPgid'}], 'help': 'force recovery of <pgid> first', 'module': 'pg', 'perm': 'rw', 'flags': 8}
cmd015: {'sig': ['pg', 'force-backfill', {'n': 'N', 'name': 'pgid', 'type': 'CephPgid'}], 'help': 'force backfill of <pgid> first', 'module': 'pg', 'perm': 'rw', 'flags': 8}
cmd016: {'sig': ['pg', 'cancel-force-recovery', {'n': 'N', 'name': 'pgid', 'type': 'CephPgid'}], 'help': 'restore normal recovery priority of <pgid>', 'module': 'pg', 'perm': 'rw', 'flags': 8}
cmd017: {'sig': ['pg', 'cancel-force-backfill', {'n': 'N', 'name': 'pgid', 'type': 'CephPgid'}], 'help': 'restore normal backfill priority of <pgid>', 'module': 'pg', 'perm': 'rw', 'flags': 8}


cmd230: {'sig': ['device', 'query-daemon-health-metrics', {'name': 'who', 'type': 'CephString'}], 'help': 'Get device health metrics for a given daemon', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd231: {'sig': ['device', 'scrape-daemon-health-metrics', {'name': 'who', 'type': 'CephString'}], 'help': 'Scrape and store device health metrics for a given daemon', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd232: {'sig': ['device', 'scrape-health-metrics', {'name': 'devid', 'req': 'False', 'type': 'CephString'}], 'help': 'Scrape and store health metrics', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd233: {'sig': ['device', 'get-health-metrics', {'name': 'devid', 'type': 'CephString'}, {'name': 'sample', 'req': 'False', 'type': 'CephString'}], 'help': 'Show stored device metrics for the device', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd234: {'sig': ['device', 'check-health'], 'help': 'Check life expectancy of devices', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd235: {'sig': ['device', 'monitoring', 'on'], 'help': 'Enable device health monitoring', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd236: {'sig': ['device', 'monitoring', 'off'], 'help': 'Disable device health monitoring', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd237: {'sig': ['device', 'predict-life-expectancy', {'name': 'devid', 'req': 'true', 'type': 'CephString'}], 'help': 'Predict life expectancy with local predictor', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd044: {'sig': ['device', 'ls'], 'help': 'Show devices', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd045: {'sig': ['device', 'info', {'name': 'devid', 'type': 'CephString'}], 'help': 'Show information about a device', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd046: {'sig': ['device', 'ls-by-daemon', {'name': 'who', 'type': 'CephString'}], 'help': 'Show devices associated with a daemon', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd047: {'sig': ['device', 'ls-by-host', {'name': 'host', 'type': 'CephString'}], 'help': 'Show devices on a host', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd048: {'sig': ['device', 'set-life-expectancy', {'name': 'devid', 'type': 'CephString'}, {'name': 'from', 'type': 'CephString'}, {'name': 'to', 'req': 'False', 'type': 'CephString'}], 'help': 'Set predicted device life expectancy', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd049: {'sig': ['device', 'rm-life-expectancy', {'name': 'devid', 'type': 'CephString'}], 'help': 'Clear predicted device life expectancy', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd244: {'sig': ['device', 'ls-lights'], 'help': 'List currently active device indicator lights', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd245: {'sig': ['device', 'light', {'name': 'enable', 'strings': 'on|off', 'type': 'CephChoices'}, {'name': 'devid', 'type': 'CephString'}, {'name': 'light_type', 'req': 'false', 'strings': 'ident|fault', 'type': 'CephChoices'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}], 'help': 'Enable or disable the device light. Default type is `ident`\nUsage: device light (on|off) <devid> [ident|fault] [--force]', 'module': 'mgr', 'perm': 'rw', 'flags': 8}


cmd040: {'sig': ['service', 'dump'], 'help': 'dump service map', 'module': 'service', 'perm': 'r', 'flags': 8}
cmd041: {'sig': ['service', 'status'], 'help': 'dump service state', 'module': 'service', 'perm': 'r', 'flags': 8}



cmd042: {'sig': ['config', 'show', {'name': 'who', 'type': 'CephString'}, {'name': 'key', 'req': 'False', 'type': 'CephString'}], 'help': 'Show running configuration', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd043: {'sig': ['config', 'show-with-defaults', {'name': 'who', 'type': 'CephString'}], 'help': 'Show running configuration (including compiled-in defaults)', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd050: {'sig': ['alerts', 'send'], 'help': '(re)send alerts immediately', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd051: {'sig': ['balancer', 'status'], 'help': 'Show balancer status', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd052: {'sig': ['balancer', 'mode', {'name': 'mode', 'strings': 'none|crush-compat|upmap', 'type': 'CephChoices'}], 'help': 'Set balancer mode', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd053: {'sig': ['balancer', 'on'], 'help': 'Enable automatic balancing', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd054: {'sig': ['balancer', 'off'], 'help': 'Disable automatic balancing', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd055: {'sig': ['balancer', 'pool', 'ls'], 'help': 'List automatic balancing pools. Note that empty list means all existing pools will be automatic balancing targets, which is the default behaviour of balancer.', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd056: {'sig': ['balancer', 'pool', 'add', {'n': 'N', 'name': 'pools', 'type': 'CephString'}], 'help': 'Enable automatic balancing for specific pools', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd057: {'sig': ['balancer', 'pool', 'rm', {'n': 'N', 'name': 'pools', 'type': 'CephString'}], 'help': 'Disable automatic balancing for specific pools', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd058: {'sig': ['balancer', 'eval', {'name': 'option', 'req': 'false', 'type': 'CephString'}], 'help': 'Evaluate data distribution for the current cluster or specific pool or specific plan', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd059: {'sig': ['balancer', 'eval-verbose', {'name': 'option', 'req': 'false', 'type': 'CephString'}], 'help': 'Evaluate data distribution for the current cluster or specific pool or specific plan (verbosely)', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd060: {'sig': ['balancer', 'optimize', {'name': 'plan', 'type': 'CephString'}, {'n': 'N', 'name': 'pools', 'req': 'false', 'type': 'CephString'}], 'help': 'Run optimizer to create a new plan', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd061: {'sig': ['balancer', 'show', {'name': 'plan', 'type': 'CephString'}], 'help': 'Show details of an optimization plan', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd062: {'sig': ['balancer', 'rm', {'name': 'plan', 'type': 'CephString'}], 'help': 'Discard an optimization plan', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd063: {'sig': ['balancer', 'reset'], 'help': 'Discard all optimization plans', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd064: {'sig': ['balancer', 'dump', {'name': 'plan', 'type': 'CephString'}], 'help': 'Show an optimization plan', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd065: {'sig': ['balancer', 'ls'], 'help': 'List all plans', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd066: {'sig': ['balancer', 'execute', {'name': 'plan', 'type': 'CephString'}], 'help': 'Execute an optimization plan', 'module': 'mgr', 'perm': 'rw', 'flags': 8}



cmd067: {'sig': ['crash', 'info', {'name': 'id', 'type': 'CephString'}], 'help': 'show crash dump metadata', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd068: {'sig': ['crash', 'ls'], 'help': 'Show new and archived crash dumps', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd069: {'sig': ['crash', 'ls-new'], 'help': 'Show new crash dumps', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd070: {'sig': ['crash', 'post'], 'help': 'Add a crash dump (use -i <jsonfile>)', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd071: {'sig': ['crash', 'prune', {'name': 'keep', 'type': 'CephString'}], 'help': 'Remove crashes older than <keep> days', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd072: {'sig': ['crash', 'rm', {'name': 'id', 'type': 'CephString'}], 'help': 'Remove a saved crash <id>', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd073: {'sig': ['crash', 'stat'], 'help': 'Summarize recorded crashes', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd074: {'sig': ['crash', 'json_report', {'name': 'hours', 'type': 'CephString'}], 'help': 'Crashes in the last <hours> hours', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd075: {'sig': ['crash', 'archive', {'name': 'id', 'type': 'CephString'}], 'help': 'Acknowledge a crash and silence health warning(s)', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd076: {'sig': ['crash', 'archive-all'], 'help': 'Acknowledge all new crashes and silence health warning(s)', 'module': 'mgr', 'perm': 'w', 'flags': 8}

cmd238: {'sig': ['influx', 'config-set', {'name': 'key', 'type': 'CephString'}, {'name': 'value', 'type': 'CephString'}], 'help': 'Set a configuration value', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd239: {'sig': ['influx', 'config-show'], 'help': 'Show current configuration', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd240: {'sig': ['influx', 'send'], 'help': 'Force sending data to Influx', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd241: {'sig': ['insights'], 'help': 'Retrieve insights report', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd242: {'sig': ['insights', 'prune-health', {'name': 'hours', 'type': 'CephString'}], 'help': 'Remove health history older than <hours> hours', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd243: {'sig': ['iostat'], 'help': 'Get IO rates', 'module': 'mgr', 'perm': 'r', 'flags': 24}

cmd289: {'sig': ['progress'], 'help': 'Show progress of recovery operations', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd290: {'sig': ['progress', 'json'], 'help': 'Show machine readable progress information', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd291: {'sig': ['progress', 'clear'], 'help': 'Reset progress tracking', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd292: {'sig': ['progress', 'on'], 'help': 'Enable progress tracking', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd293: {'sig': ['progress', 'off'], 'help': 'Disable progress tracking', 'module': 'mgr', 'perm': 'rw', 'flags': 8}


cmd295: {'sig': ['rbd', 'mirror', 'snapshot', 'schedule', 'add', {'name': 'level_spec', 'type': 'CephString'}, {'name': 'interval', 'type': 'CephString'}, {'name': 'start_time', 'req': 'false', 'type': 'CephString'}], 'help': 'Add rbd mirror snapshot schedule', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd296: {'sig': ['rbd', 'mirror', 'snapshot', 'schedule', 'remove', {'name': 'level_spec', 'type': 'CephString'}, {'name': 'interval', 'req': 'false', 'type': 'CephString'}, {'name': 'start_time', 'req': 'false', 'type': 'CephString'}], 'help': 'Remove rbd mirror snapshot schedule', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd297: {'sig': ['rbd', 'mirror', 'snapshot', 'schedule', 'list', {'name': 'level_spec', 'req': 'false', 'type': 'CephString'}], 'help': 'List rbd mirror snapshot schedule', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd298: {'sig': ['rbd', 'mirror', 'snapshot', 'schedule', 'status', {'name': 'level_spec', 'req': 'false', 'type': 'CephString'}], 'help': 'Show rbd mirror snapshot schedule status', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd299: {'sig': ['rbd', 'perf', 'image', 'stats', {'name': 'pool_spec', 'req': 'false', 'type': 'CephString'}, {'name': 'sort_by', 'req': 'false', 'strings': 'write_ops|write_bytes|write_latency|read_ops|read_bytes|read_latency', 'type': 'CephChoices'}], 'help': 'Retrieve current RBD IO performance stats', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd300: {'sig': ['rbd', 'perf', 'image', 'counters', {'name': 'pool_spec', 'req': 'false', 'type': 'CephString'}, {'name': 'sort_by', 'req': 'false', 'strings': 'write_ops|write_bytes|write_latency|read_ops|read_bytes|read_latency', 'type': 'CephChoices'}], 'help': 'Retrieve current RBD IO performance counters', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd301: {'sig': ['rbd', 'task', 'add', 'flatten', {'name': 'image_spec', 'type': 'CephString'}], 'help': 'Flatten a cloned image asynchronously in the background', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd302: {'sig': ['rbd', 'task', 'add', 'remove', {'name': 'image_spec', 'type': 'CephString'}], 'help': 'Remove an image asynchronously in the background', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd303: {'sig': ['rbd', 'task', 'add', 'trash', 'remove', {'name': 'image_id_spec', 'type': 'CephString'}], 'help': 'Remove an image from the trash asynchronously in the background', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd304: {'sig': ['rbd', 'task', 'add', 'migration', 'execute', {'name': 'image_spec', 'type': 'CephString'}], 'help': 'Execute an image migration asynchronously in the background', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd305: {'sig': ['rbd', 'task', 'add', 'migration', 'commit', {'name': 'image_spec', 'type': 'CephString'}], 'help': 'Commit an executed migration asynchronously in the background', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd306: {'sig': ['rbd', 'task', 'add', 'migration', 'abort', {'name': 'image_spec', 'type': 'CephString'}], 'help': 'Abort a prepared migration asynchronously in the background', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd307: {'sig': ['rbd', 'task', 'cancel', {'name': 'task_id', 'type': 'CephString'}], 'help': 'Cancel a pending or running asynchronous task', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd308: {'sig': ['rbd', 'task', 'list', {'name': 'task_id', 'req': 'false', 'type': 'CephString'}], 'help': 'List pending or running asynchronous tasks', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd309: {'sig': ['rbd', 'trash', 'purge', 'schedule', 'add', {'name': 'level_spec', 'type': 'CephString'}, {'name': 'interval', 'type': 'CephString'}, {'name': 'start_time', 'req': 'false', 'type': 'CephString'}], 'help': 'Add rbd trash purge schedule', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd310: {'sig': ['rbd', 'trash', 'purge', 'schedule', 'remove', {'name': 'level_spec', 'type': 'CephString'}, {'name': 'interval', 'req': 'false', 'type': 'CephString'}, {'name': 'start_time', 'req': 'false', 'type': 'CephString'}], 'help': 'Remove rbd trash purge schedule', 'module': 'mgr', 'perm': 'w', 'flags': 8}
cmd311: {'sig': ['rbd', 'trash', 'purge', 'schedule', 'list', {'name': 'level_spec', 'req': 'false', 'type': 'CephString'}], 'help': 'List rbd trash purge schedule', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd312: {'sig': ['rbd', 'trash', 'purge', 'schedule', 'status', {'name': 'level_spec', 'req': 'false', 'type': 'CephString'}], 'help': 'Show rbd trash purge schedule status', 'module': 'mgr', 'perm': 'r', 'flags': 8}

cmd318: {'sig': ['mgr', 'self-test', 'run'], 'help': 'Run mgr python interface tests', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd319: {'sig': ['mgr', 'self-test', 'background', 'start', {'name': 'workload', 'type': 'CephString'}], 'help': 'Activate a background workload (one of command_spam, throw_exception)', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd320: {'sig': ['mgr', 'self-test', 'background', 'stop'], 'help': 'Stop background workload if any is running', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd321: {'sig': ['mgr', 'self-test', 'config', 'get', {'name': 'key', 'type': 'CephString'}], 'help': 'Peek at a configuration value', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd322: {'sig': ['mgr', 'self-test', 'config', 'get_localized', {'name': 'key', 'type': 'CephString'}], 'help': 'Peek at a configuration value (localized variant)', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd323: {'sig': ['mgr', 'self-test', 'remote'], 'help': 'Test inter-module calls', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd324: {'sig': ['mgr', 'self-test', 'module', {'name': 'module', 'type': 'CephString'}], 'help': "Run another module's self_test() method", 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd325: {'sig': ['mgr', 'self-test', 'health', 'set', {'name': 'checks', 'type': 'CephString'}], 'help': 'Set a health check from a JSON-formatted description.', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd326: {'sig': ['mgr', 'self-test', 'health', 'clear', {'n': 'N', 'name': 'checks', 'req': 'False', 'type': 'CephString'}], 'help': 'Clear health checks by name. If no names provided, clear all.', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd327: {'sig': ['mgr', 'self-test', 'insights_set_now_offset', {'name': 'hours', 'type': 'CephString'}], 'help': 'Set the now time for the insights module.', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd328: {'sig': ['mgr', 'self-test', 'cluster-log', {'name': 'channel', 'type': 'CephString'}, {'name': 'priority', 'type': 'CephString'}, {'name': 'message', 'type': 'CephString'}], 'help': 'Create an audit log record.', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd329: {'sig': ['mgr', 'self-test', 'python-version'], 'help': 'Query the version of the embedded Python runtime', 'module': 'mgr', 'perm': 'r', 'flags': 8}

cmd330: {'sig': ['fs', 'status', {'name': 'fs', 'req': 'false', 'type': 'CephString'}], 'help': 'Show the status of a CephFS filesystem', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd343: {'sig': ['fs', 'volume', 'ls'], 'help': 'List volumes', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd344: {'sig': ['fs', 'volume', 'create', {'name': 'name', 'type': 'CephString'}, {'name': 'placement', 'req': 'false', 'type': 'CephString'}], 'help': 'Create a CephFS volume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd345: {'sig': ['fs', 'volume', 'rm', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'yes-i-really-mean-it', 'req': 'false', 'type': 'CephString'}], 'help': 'Delete a FS volume by passing --yes-i-really-mean-it flag', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd346: {'sig': ['fs', 'subvolumegroup', 'ls', {'name': 'vol_name', 'type': 'CephString'}], 'help': 'List subvolumegroups', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd347: {'sig': ['fs', 'subvolumegroup', 'create', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'type': 'CephString'}, {'name': 'pool_layout', 'req': 'false', 'type': 'CephString'}, {'name': 'uid', 'req': 'false', 'type': 'CephInt'}, {'name': 'gid', 'req': 'false', 'type': 'CephInt'}, {'name': 'mode', 'req': 'false', 'type': 'CephString'}], 'help': 'Create a CephFS subvolume group in a volume, and optionally, with a specific data pool layout, and a specific numeric mode', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd348: {'sig': ['fs', 'subvolumegroup', 'rm', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}], 'help': 'Delete a CephFS subvolume group in a volume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd349: {'sig': ['fs', 'subvolume', 'ls', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'List subvolumes', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd350: {'sig': ['fs', 'subvolume', 'create', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'size', 'req': 'false', 'type': 'CephInt'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}, {'name': 'pool_layout', 'req': 'false', 'type': 'CephString'}, {'name': 'uid', 'req': 'false', 'type': 'CephInt'}, {'name': 'gid', 'req': 'false', 'type': 'CephInt'}, {'name': 'mode', 'req': 'false', 'type': 'CephString'}, {'name': 'namespace_isolated', 'req': 'false', 'type': 'CephBool'}], 'help': 'Create a CephFS subvolume in a volume, and optionally, with a specific size (in bytes), a specific data pool layout, a specific mode, in a specific subvolume group and in separate RADOS namespace', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd351: {'sig': ['fs', 'subvolume', 'rm', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}, {'name': 'retain_snapshots', 'req': 'false', 'type': 'CephBool'}], 'help': 'Delete a CephFS subvolume in a volume, and optionally, in a specific subvolume group, force deleting a cancelled or failed clone, and retaining existing subvolume snapshots', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd352: {'sig': ['fs', 'subvolume', 'authorize', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'auth_id', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}, {'name': 'access_level', 'req': 'false', 'type': 'CephString'}, {'name': 'tenant_id', 'req': 'false', 'type': 'CephString'}, {'name': 'allow_existing_id', 'req': 'false', 'type': 'CephBool'}], 'help': 'Allow a cephx auth ID access to a subvolume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd353: {'sig': ['fs', 'subvolume', 'deauthorize', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'auth_id', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Deny a cephx auth ID access to a subvolume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd354: {'sig': ['fs', 'subvolume', 'authorized_list', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'List auth IDs that have access to a subvolume', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd355: {'sig': ['fs', 'subvolume', 'evict', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'auth_id', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Evict clients based on auth IDs and subvolume mounted', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd356: {'sig': ['fs', 'subvolumegroup', 'getpath', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'type': 'CephString'}], 'help': 'Get the mountpath of a CephFS subvolume group in a volume', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd357: {'sig': ['fs', 'subvolume', 'getpath', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Get the mountpath of a CephFS subvolume in a volume, and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd358: {'sig': ['fs', 'subvolume', 'info', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Get the metadata of a CephFS subvolume in a volume, and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd359: {'sig': ['fs', 'subvolumegroup', 'pin', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'true', 'type': 'CephString'}, {'name': 'pin_type', 'strings': 'export|distributed|random', 'type': 'CephChoices'}, {'name': 'pin_setting', 'req': 'true', 'type': 'CephString'}], 'help': 'Set MDS pinning policy for subvolumegroup', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd360: {'sig': ['fs', 'subvolumegroup', 'snapshot', 'ls', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'type': 'CephString'}], 'help': 'List subvolumegroup snapshots', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd361: {'sig': ['fs', 'subvolumegroup', 'snapshot', 'create', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}], 'help': 'Create a snapshot of a CephFS subvolume group in a volume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd362: {'sig': ['fs', 'subvolumegroup', 'snapshot', 'rm', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'group_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}], 'help': 'Delete a snapshot of a CephFS subvolume group in a volume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd363: {'sig': ['fs', 'subvolume', 'snapshot', 'ls', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'List subvolume snapshots', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd364: {'sig': ['fs', 'subvolume', 'snapshot', 'create', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Create a snapshot of a CephFS subvolume in a volume, and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd365: {'sig': ['fs', 'subvolume', 'snapshot', 'info', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Get the metadata of a CephFS subvolume snapshot and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd366: {'sig': ['fs', 'subvolume', 'snapshot', 'rm', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}], 'help': 'Delete a snapshot of a CephFS subvolume in a volume, and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd367: {'sig': ['fs', 'subvolume', 'resize', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'new_size', 'req': 'true', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}, {'name': 'no_shrink', 'req': 'false', 'type': 'CephBool'}], 'help': 'Resize a CephFS subvolume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd368: {'sig': ['fs', 'subvolume', 'pin', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'pin_type', 'strings': 'export|distributed|random', 'type': 'CephChoices'}, {'name': 'pin_setting', 'req': 'true', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Set MDS pinning policy for subvolume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd369: {'sig': ['fs', 'subvolume', 'snapshot', 'protect', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': '(deprecated) Protect snapshot of a CephFS subvolume in a volume, and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd370: {'sig': ['fs', 'subvolume', 'snapshot', 'unprotect', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': '(deprecated) Unprotect a snapshot of a CephFS subvolume in a volume, and optionally, in a specific subvolume group', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd371: {'sig': ['fs', 'subvolume', 'snapshot', 'clone', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'sub_name', 'type': 'CephString'}, {'name': 'snap_name', 'type': 'CephString'}, {'name': 'target_sub_name', 'type': 'CephString'}, {'name': 'pool_layout', 'req': 'false', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}, {'name': 'target_group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Clone a snapshot to target subvolume', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd372: {'sig': ['fs', 'clone', 'status', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'clone_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Get status on a cloned subvolume.', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd373: {'sig': ['fs', 'clone', 'cancel', {'name': 'vol_name', 'type': 'CephString'}, {'name': 'clone_name', 'type': 'CephString'}, {'name': 'group_name', 'req': 'false', 'type': 'CephString'}], 'help': 'Cancel an pending or ongoing clone operation.', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd374: {'sig': ['nfs', 'export', 'create', 'cephfs', {'name': 'fsname', 'type': 'CephString'}, {'name': 'clusterid', 'type': 'CephString'}, {'name': 'binding', 'type': 'CephString'}, {'name': 'readonly', 'req': 'false', 'type': 'CephBool'}, {'name': 'path', 'req': 'false', 'type': 'CephString'}], 'help': 'Create a cephfs export', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd375: {'sig': ['nfs', 'export', 'delete', {'name': 'clusterid', 'type': 'CephString'}, {'name': 'binding', 'type': 'CephString'}], 'help': 'Delete a cephfs export', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd376: {'sig': ['nfs', 'export', 'ls', {'name': 'clusterid', 'type': 'CephString'}, {'name': 'detailed', 'req': 'false', 'type': 'CephBool'}], 'help': 'List exports of a NFS cluster', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd377: {'sig': ['nfs', 'export', 'get', {'name': 'clusterid', 'type': 'CephString'}, {'name': 'binding', 'type': 'CephString'}], 'help': 'Fetch a export of a NFS cluster given the pseudo path/binding', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd378: {'sig': ['nfs', 'cluster', 'create', {'name': 'type', 'type': 'CephString'}, {'goodchars': '[A-Za-z0-9-_.]', 'name': 'clusterid', 'type': 'CephString'}, {'name': 'placement', 'req': 'false', 'type': 'CephString'}], 'help': 'Create an NFS Cluster', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd379: {'sig': ['nfs', 'cluster', 'update', {'name': 'clusterid', 'type': 'CephString'}, {'name': 'placement', 'type': 'CephString'}], 'help': 'Updates an NFS Cluster', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd380: {'sig': ['nfs', 'cluster', 'delete', {'name': 'clusterid', 'type': 'CephString'}], 'help': 'Deletes an NFS Cluster', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd381: {'sig': ['nfs', 'cluster', 'ls'], 'help': 'List NFS Clusters', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd382: {'sig': ['nfs', 'cluster', 'info', {'name': 'clusterid', 'req': 'false', 'type': 'CephString'}], 'help': 'Displays NFS Cluster info', 'module': 'mgr', 'perm': 'r', 'flags': 8}
cmd383: {'sig': ['nfs', 'cluster', 'config', 'set', {'name': 'clusterid', 'type': 'CephString'}], 'help': 'Set NFS-Ganesha config by `-i <config_file>`', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd384: {'sig': ['nfs', 'cluster', 'config', 'reset', {'name': 'clusterid', 'type': 'CephString'}], 'help': 'Reset NFS-Ganesha Config to default', 'module': 'mgr', 'perm': 'rw', 'flags': 8}
cmd403: {'sig': ['fs', 'authorize', {'name': 'filesystem', 'type': 'CephString'}, {'name': 'entity', 'type': 'CephString'}, {'n': 'N', 'name': 'caps', 'type': 'CephString'}], 'help': 'add auth for <entity> to access file system <filesystem> based on following directory and permissions pairs', 'module': 'auth', 'perm': 'rwx', 'flags': 0}

cmd392: {'sig': ['auth', 'export', {'name': 'entity', 'req': 'false', 'type': 'CephString'}], 'help': 'write keyring for requested entity, or master keyring if none given', 'module': 'auth', 'perm': 'rx', 'flags': 0}
cmd393: {'sig': ['auth', 'get', {'name': 'entity', 'type': 'CephString'}], 'help': 'write keyring file with requested key', 'module': 'auth', 'perm': 'rx', 'flags': 0}
cmd394: {'sig': ['auth', 'get-key', {'name': 'entity', 'type': 'CephString'}], 'help': 'display requested key', 'module': 'auth', 'perm': 'rx', 'flags': 0}
cmd395: {'sig': ['auth', 'print-key', {'name': 'entity', 'type': 'CephString'}], 'help': 'display requested key', 'module': 'auth', 'perm': 'rx', 'flags': 0}
cmd396: {'sig': ['auth', 'print_key', {'name': 'entity', 'type': 'CephString'}], 'help': 'display requested key', 'module': 'auth', 'perm': 'rx', 'flags': 0}
cmd397: {'sig': ['auth', 'list'], 'help': 'list authentication state', 'module': 'auth', 'perm': 'rx', 'flags': 4}
cmd398: {'sig': ['auth', 'ls'], 'help': 'list authentication state', 'module': 'auth', 'perm': 'rx', 'flags': 0}
cmd399: {'sig': ['auth', 'import'], 'help': 'auth import: read keyring file from -i <file>', 'module': 'auth', 'perm': 'rwx', 'flags': 0}
cmd400: {'sig': ['auth', 'add', {'name': 'entity', 'type': 'CephString'}, {'n': 'N', 'name': 'caps', 'req': 'false', 'type': 'CephString'}], 'help': 'add auth info for <entity> from input file, or random key if no input is given, and/or any caps specified in the command', 'module': 'auth', 'perm': 'rwx', 'flags': 0}
cmd401: {'sig': ['auth', 'get-or-create-key', {'name': 'entity', 'type': 'CephString'}, {'n': 'N', 'name': 'caps', 'req': 'false', 'type': 'CephString'}], 'help': 'get, or add, key for <name> from system/caps pairs specified in the command.  If key already exists, any given caps must match the existing caps for that key.', 'module': 'auth', 'perm': 'rwx', 'flags': 0}
cmd402: {'sig': ['auth', 'get-or-create', {'name': 'entity', 'type': 'CephString'}, {'n': 'N', 'name': 'caps', 'req': 'false', 'type': 'CephString'}], 'help': 'add auth info for <entity> from input file, or random key if no input given, and/or any caps specified in the command', 'module': 'auth', 'perm': 'rwx', 'flags': 0}
cmd404: {'sig': ['auth', 'caps', {'name': 'entity', 'type': 'CephString'}, {'n': 'N', 'name': 'caps', 'type': 'CephString'}], 'help': 'update caps for <name> from caps specified in the command', 'module': 'auth', 'perm': 'rwx', 'flags': 0}
cmd405: {'sig': ['auth', 'del', {'name': 'entity', 'type': 'CephString'}], 'help': 'delete all caps for <name>', 'module': 'auth', 'perm': 'rwx', 'flags': 4}
cmd406: {'sig': ['auth', 'rm', {'name': 'entity', 'type': 'CephString'}], 'help': 'remove all caps for <name>', 'module': 'auth', 'perm': 'rwx', 'flags': 0}


cmd432: {'sig': ['mds', 'stat'], 'help': 'show MDS status', 'module': 'mds', 'perm': 'r', 'flags': 32}
cmd433: {'sig': ['mds', 'dump', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'dump legacy MDS cluster info, optionally from epoch', 'module': 'mds', 'perm': 'r', 'flags': 2}
cmd435: {'sig': ['mds', 'getmap', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'get MDS map, optionally from epoch', 'module': 'mds', 'perm': 'r', 'flags': 2}
cmd436: {'sig': ['mds', 'metadata', {'name': 'who', 'req': 'false', 'type': 'CephString'}], 'help': 'fetch metadata for mds <role>', 'module': 'mds', 'perm': 'r', 'flags': 0}
cmd437: {'sig': ['mds', 'count-metadata', {'name': 'property', 'type': 'CephString'}], 'help': 'count MDSs by metadata field property', 'module': 'mds', 'perm': 'r', 'flags': 0}
cmd438: {'sig': ['mds', 'versions'], 'help': 'check running versions of MDSs', 'module': 'mds', 'perm': 'r', 'flags': 0}
cmd439: {'sig': ['mds', 'tell', {'name': 'who', 'type': 'CephString'}, {'n': 'N', 'name': 'args', 'type': 'CephString'}], 'help': 'send command to particular mds', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd440: {'sig': ['mds', 'compat', 'show'], 'help': 'show mds compatibility settings', 'module': 'mds', 'perm': 'r', 'flags': 0}
cmd441: {'sig': ['mds', 'stop', {'name': 'role', 'type': 'CephString'}], 'help': 'stop mds', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd442: {'sig': ['mds', 'deactivate', {'name': 'role', 'type': 'CephString'}], 'help': 'clean up specified MDS rank (use with `set max_mds` to shrink cluster)', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd443: {'sig': ['mds', 'ok-to-stop', {'n': 'N', 'name': 'ids', 'type': 'CephString'}], 'help': 'check whether stopping the specified MDS would reduce immediate availability', 'module': 'mds', 'perm': 'r', 'flags': 0}
cmd444: {'sig': ['mds', 'set_max_mds', {'name': 'maxmds', 'range': '0', 'type': 'CephInt'}], 'help': 'set max MDS index', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd445: {'sig': ['mds', 'set', {'name': 'var', 'strings': 'max_mds|max_file_size|inline_data|allow_new_snaps|allow_multimds|allow_multimds_snaps|allow_dirfrags', 'type': 'CephChoices'}, {'name': 'val', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set mds parameter <var> to <val>', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd446: {'sig': ['mds', 'freeze', {'name': 'role_or_gid', 'type': 'CephString'}, {'name': 'val', 'type': 'CephString'}], 'help': 'freeze MDS yes/no', 'module': 'mds', 'perm': 'rw', 'flags': 32}
cmd447: {'sig': ['mds', 'set_state', {'name': 'gid', 'range': '0', 'type': 'CephInt'}, {'name': 'state', 'range': '0|20', 'type': 'CephInt'}], 'help': 'set mds state of <gid> to <numeric-state>', 'module': 'mds', 'perm': 'rw', 'flags': 32}
cmd448: {'sig': ['mds', 'fail', {'name': 'role_or_gid', 'type': 'CephString'}], 'help': 'Mark MDS failed: trigger a failover if a standby is available', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd449: {'sig': ['mds', 'repaired', {'name': 'role', 'type': 'CephString'}], 'help': 'mark a damaged MDS rank as no longer damaged', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd450: {'sig': ['mds', 'rm', {'name': 'gid', 'range': '0', 'type': 'CephInt'}], 'help': 'remove nonactive mds', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd451: {'sig': ['mds', 'rmfailed', {'name': 'role', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'remove failed rank', 'module': 'mds', 'perm': 'rw', 'flags': 32}
cmd452: {'sig': ['mds', 'cluster_down'], 'help': 'take MDS cluster down', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd453: {'sig': ['mds', 'cluster_up'], 'help': 'bring MDS cluster up', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd454: {'sig': ['mds', 'compat', 'rm_compat', {'name': 'feature', 'range': '0', 'type': 'CephInt'}], 'help': 'remove compatible feature', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd455: {'sig': ['mds', 'compat', 'rm_incompat', {'name': 'feature', 'range': '0', 'type': 'CephInt'}], 'help': 'remove incompatible feature', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd456: {'sig': ['mds', 'add_data_pool', {'name': 'pool', 'type': 'CephString'}], 'help': 'add data pool <pool>', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd457: {'sig': ['mds', 'rm_data_pool', {'name': 'pool', 'type': 'CephString'}], 'help': 'remove data pool <pool>', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd458: {'sig': ['mds', 'remove_data_pool', {'name': 'pool', 'type': 'CephString'}], 'help': 'remove data pool <pool>', 'module': 'mds', 'perm': 'rw', 'flags': 2}
cmd459: {'sig': ['mds', 'newfs', {'name': 'metadata', 'range': '0', 'type': 'CephInt'}, {'name': 'data', 'range': '0', 'type': 'CephInt'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'make new filesystem using pools <metadata> and <data>', 'module': 'mds', 'perm': 'rw', 'flags': 2}


cmd434: {'sig': ['fs', 'dump', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'dump all CephFS status, optionally from epoch', 'module': 'mds', 'perm': 'r', 'flags': 0}
cmd460: {'sig': ['fs', 'new', {'name': 'fs_name', 'type': 'CephString'}, {'name': 'metadata', 'type': 'CephString'}, {'name': 'data', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}, {'name': 'allow_dangerous_metadata_overlay', 'req': 'false', 'type': 'CephBool'}], 'help': 'make new filesystem using named pools <metadata> and <data>', 'module': 'fs', 'perm': 'rw', 'flags': 0}
cmd461: {'sig': ['fs', 'fail', {'name': 'fs_name', 'type': 'CephString'}], 'help': 'bring the file system down and all of its ranks', 'module': 'fs', 'perm': 'rw', 'flags': 0}
cmd462: {'sig': ['fs', 'rm', {'name': 'fs_name', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'disable the named filesystem', 'module': 'fs', 'perm': 'rw', 'flags': 0}
cmd463: {'sig': ['fs', 'reset', {'name': 'fs_name', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'disaster recovery only: reset to a single-MDS map', 'module': 'fs', 'perm': 'rw', 'flags': 0}
cmd464: {'sig': ['fs', 'ls'], 'help': 'list filesystems', 'module': 'fs', 'perm': 'r', 'flags': 0}
cmd465: {'sig': ['fs', 'get', {'name': 'fs_name', 'type': 'CephString'}], 'help': 'get info about one filesystem', 'module': 'fs', 'perm': 'r', 'flags': 0}
cmd466: {'sig': ['fs', 'set', {'name': 'fs_name', 'type': 'CephString'}, {'name': 'var', 'strings': 'max_mds|max_file_size|allow_new_snaps|inline_data|cluster_down|allow_dirfrags|balancer|standby_count_wanted|session_timeout|session_autoclose|allow_standby_replay|down|joinable|min_compat_client', 'type': 'CephChoices'}, {'name': 'val', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}, {'name': 'yes_i_really_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'set fs parameter <var> to <val>', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd467: {'sig': ['fs', 'flag', 'set', {'name': 'flag_name', 'strings': 'enable_multiple', 'type': 'CephChoices'}, {'name': 'val', 'type': 'CephString'}, {'name': 'yes_i_really_mean_it', 'req': 'false', 'type': 'CephBool'}], 'help': 'Set a global CephFS flag', 'module': 'fs', 'perm': 'rw', 'flags': 0}
cmd468: {'sig': ['fs', 'add_data_pool', {'name': 'fs_name', 'type': 'CephString'}, {'name': 'pool', 'type': 'CephString'}], 'help': 'add data pool <pool>', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd469: {'sig': ['fs', 'rm_data_pool', {'name': 'fs_name', 'type': 'CephString'}, {'name': 'pool', 'type': 'CephString'}], 'help': 'remove data pool <pool>', 'module': 'mds', 'perm': 'rw', 'flags': 0}
cmd470: {'sig': ['fs', 'set_default', {'name': 'fs_name', 'type': 'CephString'}], 'help': 'set the default to the named filesystem', 'module': 'fs', 'perm': 'rw', 'flags': 4}
cmd471: {'sig': ['fs', 'set-default', {'name': 'fs_name', 'type': 'CephString'}], 'help': 'set the default to the named filesystem', 'module': 'fs', 'perm': 'rw', 'flags': 0}

cmd633: {'sig': ['mgr', 'dump', {'name': 'epoch', 'range': '0', 'req': 'false', 'type': 'CephInt'}], 'help': 'dump the latest MgrMap', 'module': 'mgr', 'perm': 'r', 'flags': 0}
cmd634: {'sig': ['mgr', 'fail', {'name': 'who', 'req': 'false', 'type': 'CephString'}], 'help': 'treat the named manager daemon as failed', 'module': 'mgr', 'perm': 'rw', 'flags': 0}
cmd635: {'sig': ['mgr', 'module', 'ls'], 'help': 'list active mgr modules', 'module': 'mgr', 'perm': 'r', 'flags': 0}
cmd636: {'sig': ['mgr', 'services'], 'help': 'list service endpoints provided by mgr modules', 'module': 'mgr', 'perm': 'r', 'flags': 0}
cmd637: {'sig': ['mgr', 'module', 'enable', {'name': 'module', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'strings': '--force', 'type': 'CephChoices'}], 'help': 'enable mgr module', 'module': 'mgr', 'perm': 'rw', 'flags': 0}
cmd638: {'sig': ['mgr', 'module', 'disable', {'name': 'module', 'type': 'CephString'}], 'help': 'disable mgr module', 'module': 'mgr', 'perm': 'rw', 'flags': 0}
cmd639: {'sig': ['mgr', 'metadata', {'name': 'who', 'req': 'false', 'type': 'CephString'}], 'help': 'dump metadata for all daemons or a specific daemon', 'module': 'mgr', 'perm': 'r', 'flags': 0}
cmd640: {'sig': ['mgr', 'count-metadata', {'name': 'property', 'type': 'CephString'}], 'help': 'count ceph-mgr daemons by metadata field property', 'module': 'mgr', 'perm': 'r', 'flags': 0}
cmd641: {'sig': ['mgr', 'versions'], 'help': 'check running versions of ceph-mgr daemons', 'module': 'mgr', 'perm': 'r', 'flags': 0}


cmd642: {'sig': ['config', 'set', {'name': 'who', 'type': 'CephString'}, {'name': 'name', 'type': 'CephString'}, {'name': 'value', 'type': 'CephString'}, {'name': 'force', 'req': 'false', 'type': 'CephBool'}], 'help': 'Set a configuration option for one or more entities', 'module': 'config', 'perm': 'rw', 'flags': 0}
cmd643: {'sig': ['config', 'rm', {'name': 'who', 'type': 'CephString'}, {'name': 'name', 'type': 'CephString'}], 'help': 'Clear a configuration option for one or more entities', 'module': 'config', 'perm': 'rw', 'flags': 0}
cmd644: {'sig': ['config', 'get', {'name': 'who', 'type': 'CephString'}, {'name': 'key', 'req': 'False', 'type': 'CephString'}], 'help': 'Show configuration option(s) for an entity', 'module': 'config', 'perm': 'r', 'flags': 0}
cmd645: {'sig': ['config', 'dump'], 'help': 'Show all configuration option(s)', 'module': 'mon', 'perm': 'r', 'flags': 0}
cmd646: {'sig': ['config', 'help', {'name': 'key', 'type': 'CephString'}], 'help': 'Describe a configuration option', 'module': 'config', 'perm': 'r', 'flags': 0}
cmd647: {'sig': ['config', 'ls'], 'help': 'List available configuration options', 'module': 'config', 'perm': 'r', 'flags': 0}
cmd648: {'sig': ['config', 'assimilate-conf'], 'help': 'Assimilate options from a conf, and return a new, minimal conf file', 'module': 'config', 'perm': 'rw', 'flags': 0}
cmd649: {'sig': ['config', 'log', {'name': 'num', 'req': 'False', 'type': 'CephInt'}], 'help': 'Show recent history of config changes', 'module': 'config', 'perm': 'r', 'flags': 0}
cmd650: {'sig': ['config', 'reset', {'name': 'num', 'range': '0', 'type': 'CephInt'}], 'help': 'Revert configuration to a historical version specified by <num>', 'module': 'config', 'perm': 'rw', 'flags': 0}
cmd651: {'sig': ['config', 'generate-minimal-conf'], 'help': 'Generate a minimal ceph.conf file', 'module': 'config', 'perm': 'r', 'flags': 0}


cmd624: {'sig': ['config-key', 'get', {'name': 'key', 'type': 'CephString'}], 'help': 'get <key>', 'module': 'config-key', 'perm': 'r', 'flags': 0}
cmd625: {'sig': ['config-key', 'set', {'name': 'key', 'type': 'CephString'}, {'name': 'val', 'req': 'false', 'type': 'CephString'}], 'help': 'set <key> to value <val>', 'module': 'config-key', 'perm': 'rw', 'flags': 0}
cmd626: {'sig': ['config-key', 'put', {'name': 'key', 'type': 'CephString'}, {'name': 'val', 'req': 'false', 'type': 'CephString'}], 'help': 'put <key>, value <val>', 'module': 'config-key', 'perm': 'rw', 'flags': 4}
cmd627: {'sig': ['config-key', 'del', {'name': 'key', 'type': 'CephString'}], 'help': 'delete <key>', 'module': 'config-key', 'perm': 'rw', 'flags': 4}
cmd628: {'sig': ['config-key', 'rm', {'name': 'key', 'type': 'CephString'}], 'help': 'rm <key>', 'module': 'config-key', 'perm': 'rw', 'flags': 0}
cmd629: {'sig': ['config-key', 'exists', {'name': 'key', 'type': 'CephString'}], 'help': "check for <key>'s existence", 'module': 'config-key', 'perm': 'r', 'flags': 0}
cmd630: {'sig': ['config-key', 'list'], 'help': 'list keys', 'module': 'config-key', 'perm': 'r', 'flags': 4}
cmd631: {'sig': ['config-key', 'ls'], 'help': 'list keys', 'module': 'config-key', 'perm': 'r', 'flags': 0}
cmd632: {'sig': ['config-key', 'dump', {'name': 'key', 'req': 'false', 'type': 'CephString'}], 'help': 'dump keys and values (with optional prefix)', 'module': 'config-key', 'perm': 'r', 'flags': 0}
